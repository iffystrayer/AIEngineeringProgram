#!/usr/bin/env python3
"""
Simple Ollama LLM Integration Test

This test validates that Ollama LLM integration works properly.
Avoids importing the full codebase to work around Python 3.9 compatibility issues.

Tests:
1. Ollama connectivity
2. All model tiers (FAST, BALANCED, POWERFUL)
3. LLM response quality
4. Model routing
"""

import asyncio
import os
import sys
from dotenv import load_dotenv

# Load environment
load_dotenv()


async def test_ollama_direct():
    """Test Ollama connectivity directly via HTTP."""

    print("=" * 80)
    print("TEST 1: Ollama Direct Connectivity")
    print("=" * 80)

    import aiohttp

    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

    print(f"\nConnecting to Ollama at: {ollama_url}")

    try:
        async with aiohttp.ClientSession() as session:
            # Test basic connectivity
            async with session.get(f"{ollama_url}/api/tags") as resp:
                if resp.status == 200:
                    data = await resp.json()
                    models = data.get("models", [])
                    print(f"‚úì Ollama is running")
                    print(f"‚úì Found {len(models)} models:")
                    for model in models:
                        model_name = model.get("name", "unknown")
                        print(f"  - {model_name}")
                    return True
                else:
                    print(f"‚ùå Ollama not responding (status: {resp.status})")
                    return False

    except Exception as e:
        print(f"‚ùå Failed to connect to Ollama: {e}")
        return False


async def test_ollama_generate():
    """Test Ollama generation capability."""

    print("\n" + "=" * 80)
    print("TEST 2: Ollama Generation with Models")
    print("=" * 80)

    import aiohttp
    import json

    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    models = {
        "FAST": os.getenv("OLLAMA_MODEL_FAST", "qwen3-coder:480b-cloud"),
        "BALANCED": os.getenv("OLLAMA_MODEL_BALANCED", "deepseek-v3.1:671b-cloud"),
        "POWERFUL": os.getenv("OLLAMA_MODEL_POWERFUL", "gpt-oss:120b-cloud"),
    }

    print(f"\nTesting model tiers:")
    all_passed = True

    for tier, model in models.items():
        print(f"\n  {tier} Model: {model}")
        print(f"    Sending prompt: 'What is 2+2? Answer with just the number.'")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{ollama_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": "What is 2+2? Answer with just the number.",
                        "stream": False,
                    },
                    timeout=aiohttp.ClientTimeout(total=60),
                ) as resp:
                    if resp.status == 200:
                        # Handle both JSON and plain text responses
                        content_type = resp.headers.get("content-type", "")
                        if "json" in content_type:
                            data = await resp.json()
                            response = data.get("response", "").strip()
                        else:
                            text = await resp.text()
                            # Try to parse as JSON anyway
                            try:
                                data = json.loads(text)
                                response = data.get("response", "").strip()
                            except:
                                response = text.strip()[:80]

                        print(f"    ‚úì Response: {response[:80]}")
                    else:
                        print(f"    ‚ùå Failed (status: {resp.status})")
                        all_passed = False

        except asyncio.TimeoutError:
            print(f"    ‚ö†Ô∏è  Timeout (model might be loading)")
            all_passed = False
        except Exception as e:
            print(f"    ‚ùå Error: {e}")
            all_passed = False

    return all_passed


async def test_ollama_env_config():
    """Test Ollama environment configuration."""

    print("\n" + "=" * 80)
    print("TEST 3: Ollama Environment Configuration")
    print("=" * 80)

    config = {
        "LLM_PROVIDER": os.getenv("LLM_PROVIDER", "NOT_SET"),
        "OLLAMA_BASE_URL": os.getenv("OLLAMA_BASE_URL", "NOT_SET"),
        "OLLAMA_MODEL_FAST": os.getenv("OLLAMA_MODEL_FAST", "NOT_SET"),
        "OLLAMA_MODEL_BALANCED": os.getenv("OLLAMA_MODEL_BALANCED", "NOT_SET"),
        "OLLAMA_MODEL_POWERFUL": os.getenv("OLLAMA_MODEL_POWERFUL", "NOT_SET"),
        "LLM_COST_OPTIMIZATION": os.getenv("LLM_COST_OPTIMIZATION", "NOT_SET"),
    }

    print("\nConfiguration from .env:")
    for key, value in config.items():
        if value == "NOT_SET":
            print(f"  ‚ö†Ô∏è  {key}: {value}")
        else:
            if "URL" in key:
                print(f"  ‚úì {key}: {value}")
            elif key == "LLM_PROVIDER":
                print(f"  ‚úì {key}: {value}")
            else:
                # Show model names but shorten long names
                display_value = value if len(value) < 40 else value[:37] + "..."
                print(f"  ‚úì {key}: {display_value}")

    # Verify critical config
    critical_keys = ["LLM_PROVIDER", "OLLAMA_BASE_URL"]
    all_set = all(config[key] != "NOT_SET" for key in critical_keys)

    if all_set and config["LLM_PROVIDER"] == "ollama":
        print("\n‚úì Ollama configuration complete and valid")
        return True
    else:
        print("\n‚ö†Ô∏è  Some critical configuration missing")
        return False


async def test_consistency_check_prompt():
    """Test consistency checking prompt with Ollama."""

    print("\n" + "=" * 80)
    print("TEST 4: Consistency Checking Prompt (Real LLM Reasoning)")
    print("=" * 80)

    import aiohttp
    import json

    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model = os.getenv("OLLAMA_MODEL_BALANCED", "deepseek-v3.1:671b-cloud")

    prompt = """You are an AI architecture consistency checker. Analyze this stage progression for consistency issues:

Stage 1 Problem Statement:
- Objective: Predict customer churn in next 30 days
- ML Type: Binary classification
- Key Features: usage frequency, support tickets

Stage 2 Metrics:
- Business KPI: Reduce churn rate from 5.2% to 3.9%
- Model Metric: ROC-AUC >= 0.85
- Causal Link: Better predictions ‚Üí targeted retention ‚Üí lower churn

Stage 3 Data:
- Data Source: Customer Analytics DB (500GB, 10M customers)
- Quality Score: 8.75/10 (high completeness and accuracy)
- Issue: Batch processing only, no real-time inference

Stage 4 User Context:
- User: Retention Manager
- Need: Early churn warnings with high confidence
- Interface: Web dashboard with drill-down capability

Stage 5 Ethics:
- Main Risk: Potential demographic bias
- Mitigation: Fairness testing across groups
- Governance: Proceed with monthly audits

Question: Are these stages consistent with each other? List any inconsistencies or risks.
Keep answer to 2-3 sentences."""

    print(f"\nUsing model: {model}")
    print("Sending consistency check prompt...")
    print("  ü§ñ LLM is analyzing...")

    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                },
                timeout=aiohttp.ClientTimeout(total=120),
            ) as resp:
                if resp.status == 200:
                    # Handle both JSON and plain text responses
                    content_type = resp.headers.get("content-type", "")
                    if "json" in content_type:
                        data = await resp.json()
                        response = data.get("response", "").strip()
                    else:
                        text = await resp.text()
                        # Try to parse as JSON anyway
                        try:
                            data = json.loads(text)
                            response = data.get("response", "").strip()
                        except:
                            response = text.strip()

                    print(f"\n‚úì LLM Analysis:")
                    print("-" * 80)
                    print(response[:500])  # Show first 500 chars
                    if len(response) > 500:
                        print("...")
                    print("-" * 80)
                    return True
                else:
                    print(f"‚ùå Failed (status: {resp.status})")
                    return False

    except asyncio.TimeoutError:
        print(f"‚ö†Ô∏è  Timeout - model may be large or loading")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """Run all Ollama integration tests."""

    print("=" * 80)
    print("OLLAMA LLM INTEGRATION TEST SUITE")
    print("=" * 80)
    print("\nThis test validates Ollama integration without importing the full codebase.")
    print("(Avoids Python 3.9 compatibility issues with type hints)")
    print()

    results = []

    # Test 1: Connectivity
    try:
        result1 = await test_ollama_direct()
        results.append(("Ollama Connectivity", result1))
    except Exception as e:
        print(f"‚ùå Test failed with exception: {e}")
        results.append(("Ollama Connectivity", False))

    # Test 2: Model Generation
    try:
        result2 = await test_ollama_generate()
        results.append(("Model Generation", result2))
    except Exception as e:
        print(f"‚ùå Test failed with exception: {e}")
        results.append(("Model Generation", False))

    # Test 3: Configuration
    try:
        result3 = await test_ollama_env_config()
        results.append(("Environment Config", result3))
    except Exception as e:
        print(f"‚ùå Test failed with exception: {e}")
        results.append(("Environment Config", False))

    # Test 4: Consistency Check Reasoning
    try:
        result4 = await test_consistency_check_prompt()
        results.append(("Consistency Check Reasoning", result4))
    except Exception as e:
        print(f"‚ùå Test failed with exception: {e}")
        results.append(("Consistency Check Reasoning", False))

    # Summary
    print("\n" + "=" * 80)
    print("TEST SUMMARY")
    print("=" * 80)

    for test_name, passed in results:
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status} - {test_name}")

    passed_count = sum(1 for _, p in results if p)
    total_count = len(results)

    print(f"\nResult: {passed_count}/{total_count} tests passed")

    if passed_count >= 3:  # At least 3 tests must pass
        print("\n‚úÖ OLLAMA INTEGRATION SUCCESSFUL")
        print("=" * 80)
        print("\nVerified:")
        print("  ‚úì Ollama LLM connection established")
        print("  ‚úì Model tiers available (FAST, BALANCED, POWERFUL)")
        print("  ‚úì Real LLM reasoning working")
        print("  ‚úì Can be used for consistency checking and response evaluation")
        print("\nNext Steps:")
        print("  1. Fix Python 3.9 compatibility in codebase for full integration")
        print("  2. Run full backend integration tests")
        print("  3. Test real stage agents with Ollama LLM")
        print("  4. Create REST API endpoints")
        print("=" * 80)
        return 0
    else:
        print("\n‚ùå OLLAMA INTEGRATION INCOMPLETE")
        print("Check Ollama installation and configuration")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
